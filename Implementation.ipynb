{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Webpages using SMCFL\n",
    "Implement the following paper - https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14582\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The WebKB dataset contains 1051 webpages from two classes (230 pages in the course class and 821 pages in the non-course class). Each webpage is characterized by \tthe page view and the link view. We use a preprocessed version of this dataset, where 3000-dimensional and 1840- dimensional original features are extracted from \tthe page view and link view of a webpage, respectively.\n",
    "\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-51/www/co-training/data/\n",
    "\n",
    "### Background\n",
    "Webpage data is often multi-view and high-dimensional, and the webpage classification application is usually semisupervised.\n",
    "Due to these characteristics, using semisupervised multi-view feature learning (SMFL) technique to deal with the webpage classification problem has recently received much attention.\n",
    "\n",
    "Webpage classification has three characteristics: \n",
    "\n",
    "1. Webpage is a kind of multi-view data since it usually contains two or more types of data, e.g.,text, hyperlinks and images, where each type of data can be \tregarded as a view. These multiple views describe the same webpage.  Multi-view learning is concerned with the problem of machine learning from data represented by \tmultiple distinct feature sets. Like in web-page classification, a web page can be described by the document text itself and at the same time by the anchor text \tattached to hyperlinks pointing to this page.\n",
    "2. Webpage classification is a semisupervised application, since labeled pages are harder to collect compared to unlabeled pages in practice. \n",
    "3. Webpage data is high-dimensional, since webpages usually contain much information. \n",
    "\n",
    "Considering these three characteristics, it is crucial to design effective semi-supervised multi-view feature learning (SMFL) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from urllib.request import urlopen \n",
    "from random import shuffle \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# These files will be used to generate features and labels\n",
    "fulltext_test_file=\"tfidf_matrix_fulltext_test_large.txt\"\n",
    "inlink_test_file=\"tfidf_matrix_inlinks_test_large.txt\"\n",
    "fulltext_train_file=\"tfidf_matrix_fulltext_train_large.txt\"\n",
    "inlink_train_file=\"tfidf_matrix_inlinks_train_large.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the below methods we generate the train and test files. The files contain TfIdf Matrices\n",
    "\n",
    "def get_tf(documents, terms):\n",
    "    doc_matrix = []\n",
    "    for itr in documents:\n",
    "        doc_terms = [itr[0].count(t) for t in terms]\n",
    "        doc_matrix.append(doc_terms)\n",
    "\n",
    "    return np.array(doc_matrix)\n",
    "\n",
    "# Convert Text to TfIdf Vector\n",
    "def get_vector(location1, view_location1, location2, view_location2, principal_components, train_file_name, test_file_name):\n",
    "    # Create train and test tfidf matrices. Then generate files for the same.\n",
    "    \n",
    "    # Declare variables for storage purpose \n",
    "    # For webpage\n",
    "    text = []\n",
    "    # For number of unique words in all samples\n",
    "    uniques = []\n",
    "    location = location1\n",
    "    # Using PorterStemmer for stemming purpose\n",
    "    pStemmer = PorterStemmer()\n",
    "    # To manage Regular Expressions\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    labels = []\n",
    "    \n",
    "    # Data Preprocessing\n",
    "    for file in os.listdir(location):\n",
    "        file = view_location1 + file\n",
    "        sock = urlopen(file) \n",
    "        htmlStr = sock.read() \n",
    "        htmlStr = htmlStr.decode(\"windows-1252\")\n",
    "        sock.close()\n",
    "        \n",
    "        labels.append(0)\n",
    "        # Get the text in <>\n",
    "        clean_reg = re.compile('<.*?>')\n",
    "        htmlStr = re.sub(clean_reg, '', htmlStr)\n",
    "        tokens = tokenizer.tokenize(htmlStr.lower())\n",
    "\n",
    "        # Use Porter Stemmer\n",
    "        words = [pStemmer.stem(line) for line in tokens if line not in '']\n",
    "        \n",
    "        # Remove the stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in words if not w in stop_words]\n",
    "        \n",
    "        # Get unique words\n",
    "        uniques += list(set(tokens))\n",
    "        uniques = list(set(uniques))\n",
    "\n",
    "        temp_str = \"\"\n",
    "        for k in tokens:\n",
    "            temp_str += k + \" \"\n",
    "\n",
    "        temp_list = [temp_str]\n",
    "        # text will hold entire webpage text\n",
    "        text.append(temp_list)\n",
    "\n",
    "    count_class1 = len(labels)  # stores count of class one samples\n",
    "    location = location2\n",
    "    # read files\n",
    "    for filename in os.listdir(location):\n",
    "        filename = view_location2 + filename\n",
    "        sock = urlopen(filename) \n",
    "        htmlStr = sock.read()\n",
    "        htmlStr = htmlStr.decode(\"windows-1252\")                           \n",
    "        sock.close()    \n",
    "        labels.append(1)\n",
    "\n",
    "        # for obtaining text inside <> tags                     \n",
    "        cleanr = re.compile('<.*?>')\n",
    "        htmlStr = re.sub(cleanr, '', htmlStr)\n",
    "\n",
    "        # preprocess the data\n",
    "        tokens = word_tokenize(htmlStr)\n",
    "        words = [pStemmer.stem(line) for line in tokens if line not in '']\n",
    "        #stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in words if not w in words]\n",
    "        uniques += list(set(tokens))\n",
    "        uniques = list(set(uniques))\n",
    "\n",
    "        temp_str = \"\"\n",
    "        for k in tokens:\n",
    "            temp_str += k + \" \"\n",
    "\n",
    "        temp_list = [temp_str]\n",
    "        text.append(temp_list)\n",
    "\n",
    "    count_class2 = len(labels) - count_class1\n",
    "    labels = np.asarray(labels)\n",
    "    labels = labels.reshape(labels.shape[0], 1)\n",
    "\n",
    "    tf_matrix = get_tf(text, uniques)    \n",
    "    tfIdf = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "    tf_idf_matrix = tfIdf.fit_transform(tf_matrix).todense() \n",
    "    \n",
    "    # Dimensionality reduction using truncated SVD\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    svd = TruncatedSVD(n_components=principal_components, random_state=42)\n",
    "    tf_idf_matrix_SVD = svd.fit_transform(tf_idf_matrix)\n",
    "\n",
    "    tfidf_with_labels = np.concatenate((tf_idf_matrix_SVD, labels), axis=1)\n",
    "    # Extract 30% for testing\n",
    "    class1_test_data = int(count_class1  * 0.3)\n",
    "    tfidf_test_matrix = tfidf_with_labels[0:class1_test_data,:]\n",
    "    class2_test_data = int(count_class2  * 0.3)\n",
    "    temp_tfidf_test_matrix = tfidf_with_labels[count_class1:(count_class1+class2_test_data),:]\n",
    "    tfidf_test_matrix = np.concatenate((tfidf_test_matrix,temp_tfidf_test_matrix),axis = 0)\n",
    "    temp_matrix1 = tfidf_with_labels[class1_test_data:count_class1,:]\n",
    "    temp_matrix2 = tfidf_with_labels[(count_class1+class2_test_data):,:]\n",
    "    tfidf_train_matrix = np.concatenate((temp_matrix1, temp_matrix2), axis=0)\n",
    "\n",
    "    # writes training tfidf into file\n",
    "    fp = open(train_file_name, 'w')\n",
    "    for i in range(tfidf_train_matrix.shape[0]):\n",
    "        for j in range(tfidf_train_matrix.shape[1]):\n",
    "            fp.write(str(tfidf_train_matrix[i][j]) + \" \")      \n",
    "        fp.write(\"\\n\")    \n",
    "    fp.close()\n",
    "\n",
    "    # writes test tfidf into file\n",
    "    fp = open(test_file_name, 'w')\n",
    "    for i in range(tfidf_test_matrix.shape[0]):\n",
    "        for j in range(tfidf_test_matrix.shape[1]):\n",
    "            fp.write(str(tfidf_test_matrix[i][j]) + \" \")\n",
    "        fp.write(\"\\n\")\n",
    "    fp.close()\n",
    "\n",
    "# Define the path to the course and non-course file location\n",
    "\n",
    "# 100 dimensions\n",
    "location1 = '/Users/ankursrivastava/Desktop/Machine Learning/IIIT/Assignments/Project/dataset/course-cotrain-data/fulltext/course/'\n",
    "view_location1 = 'file:///Users/ankursrivastava/Desktop/Machine Learning/IIIT/Assignments/Project/dataset/course-cotrain-data/fulltext/course/'\n",
    "location2 = '/Users/ankursrivastava/Desktop/Machine Learning/IIIT/Assignments/Project/dataset/course-cotrain-data/fulltext/non-course/'\n",
    "view_location2 = 'file:///Users/ankursrivastava/Desktop/Machine Learning/IIIT/Assignments/Project/dataset/course-cotrain-data/fulltext/non-course/'\n",
    "\n",
    "get_vector(location1,view_location1,location2,view_location2, 100, 'tfidf_matrix_fulltext_train_large.txt', 'tfidf_matrix_fulltext_test_large.txt')\n",
    "\n",
    "location1 = '/Users/ankursrivastava/Desktop/Machine Learning/IIIT/Assignments/Project/dataset/course-cotrain-data/inlinks/course/'\n",
    "view_location1 = 'file:///Users/ankursrivastava/Desktop/Machine Learning/IIIT/Assignments/Project/dataset/course-cotrain-data/inlinks/course/'\n",
    "location2 = '/Users/ankursrivastava/Desktop/Machine Learning/IIIT/Assignments/Project/dataset/course-cotrain-data/inlinks/non-course/'\n",
    "view_location2 = 'file:///Users/ankursrivastava/Desktop/Machine Learning/IIIT/Assignments/Project/dataset/course-cotrain-data/inlinks/non-course/'\n",
    "\n",
    "get_vector(location1,view_location1,location2,view_location2, 100, 'tfidf_matrix_inlinks_train_large.txt', 'tfidf_matrix_inlinks_test_large.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels and features from the test file\n",
    "def get_test_data(fulltext_file, inlink_file):\n",
    "    fulltext_data=np.genfromtxt(fulltext_file,dtype=None,delimiter=\" \")\n",
    "    inlink_data=np.genfromtxt(inlink_file,dtype=None,delimiter=\" \")\n",
    "    print(fulltext_data.shape)\n",
    "    print(fulltext_data)\n",
    "    \n",
    "    #extract features and label\n",
    "    text_data=fulltext_data[:,:-1]\n",
    "    text_label=fulltext_data[:,-1]\n",
    "    link_data=inlink_data[:,:-1]\n",
    "    link_label=inlink_data[:,-1]\n",
    "    print(text_data)\n",
    "    \n",
    "    #convert labels from float to int\n",
    "    text_label=text_label.astype(int)\n",
    "    link_label=link_label.astype(int)\n",
    "    print(text_label)\n",
    "    print(text_label.shape)\n",
    "\n",
    "    text_data=sklearn.preprocessing.scale(text_data)\n",
    "    link_data=sklearn.preprocessing.scale(link_date)\n",
    "    number_of_samples=text_label.shape[0]\n",
    "\n",
    "    text_label=text_label.reshape(text_label.shape[0], 1)\n",
    "    link_label=link_label.reshape(link_label.shape[0], 1)\n",
    "\n",
    "    data_full_with_label = np.concatenate((text_data, text_label), axis=1)\n",
    "    data_inlink_with_label = np.concatenate((link_data, link_label), axis=1)\n",
    "\n",
    "    class0_text=data_full_with_label[data_full_with_label[:,-1]==0][:,:-1]\n",
    "    class0_inlink=data_inlink_with_label[data_inlink_with_label[:,-1]==0][:,:-1]\n",
    "    class1_text=data_full_with_label[data_full_with_label[:,-1]==1][:,:-1]\n",
    "    class1_inlink=data_inlink_with_label[data_inlink_with_label[:,-1]==1][:,:-1]\n",
    "\n",
    "    class0 = np.stack((class0_text, class0_inlink), axis=0)\n",
    "    class1 = np.stack((class1_text, class1_inlink), axis=0)\n",
    "    print(class0.shape, class1.shape)\n",
    "    \n",
    "    #Add to View List\n",
    "    view_list=[class0,class1]\n",
    "    print(view_list[0].shape,view_list[1].shape)\n",
    "    return view_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels and features from the train file\n",
    "def get_train_data(fulltext_file, inlink_file) :\n",
    "    fulltext_data=np.genfromtxt(fulltext_file,dtype=None,delimiter=\" \")\n",
    "    inlink_data=np.genfromtxt(inlink_file,dtype=None,delimiter=\" \")\n",
    "\n",
    "    # get the features and labels from fulltext\n",
    "    text_features=fulltext_data[:,:-1]\n",
    "    text_labels=fulltext_data[:,-1]\n",
    "    \n",
    "    # get the features and labels from inlinks\n",
    "    inlink_features=inlink_data[:,:-1]\n",
    "    inlink_labels=inlink_data[:,-1]\n",
    "    \n",
    "    # convert to integer\n",
    "    text_labels=text_labels.astype(int)\n",
    "    inlink_labels=inlink_labels.astype(int)\n",
    "\n",
    "    text_features=sklearn.preprocessing.scale(text_features)\n",
    "    inlink_features=sklearn.preprocessing.scale(inlink_features)\n",
    "    count_data_samples=text_features.shape[0]\n",
    "\n",
    "    # select random data points and set label -1\n",
    "    for k in range(count_data_samples):\n",
    "        # generate a random number between 0 and 1\n",
    "        p=random.uniform(0,1)\n",
    "        if(p<0.7):\n",
    "            text_labels[k]=-1\n",
    "            inlink_labels[k]=-1\n",
    "\n",
    "    print(\"-------------------------------------\")\n",
    "\n",
    "    text_labels=text_labels.reshape(text_labels.shape[0], 1)\n",
    "    inlink_labels=inlink_labels.reshape(inlink_labels.shape[0], 1)\n",
    "\n",
    "    text_with_label = np.concatenate((text_features, text_labels), axis=1)\n",
    "    inlink_with_label = np.concatenate((inlink_features, inlink_labels), axis=1)\n",
    "\n",
    "    class0_text=text_with_label[text_with_label[:,-1]==0][:,:-1]\n",
    "    class0_inlink=inlink_labels[inlink_labels[:,-1]==0][:,:-1]\n",
    "    class1_text=text_with_label[text_with_label[:,-1]==1][:,:-1]\n",
    "    class1_inlink=inlink_with_label[inlink_with_label[:,-1]==1][:,:-1]\n",
    "    \n",
    "    unlabelled_text=text_with_label[text_with_label[:,-1]==-1][:,:-1]\n",
    "    unlabelled_inlink=inlink_with_label[inlink_with_label[:,-1]==-1][:,:-1]\n",
    "\n",
    "    class0 = np.stack((class0_text, class0_inlink), axis=0)\n",
    "    class1 = np.stack((class1_text, class1_inlink), axis=0)\n",
    "    unlabelled = np.stack((unlabelled_text, unlabelled_inlink), axis=0)\n",
    "\n",
    "    print(class0.shape, class1.shape, unlabelled.shape)\n",
    "\n",
    "    view=[class0,class1,unlabelled]\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective function is given by ==> R = max f(W) = S_w - r1*S_b - r2 * S_t\n",
    "# S_w = within-class correlation\n",
    "# S_b = between-class correlation\n",
    "# S_t = total correlation\n",
    "def calculate_R(view):\n",
    "    count=len(view)\n",
    "    feature_count=view[0].shape[2]\n",
    "    S_w=np.zeros((feature_count,feature_count))\n",
    "\n",
    "    # Calculate S_w\n",
    "    for i in range(count-1) :\n",
    "        ar_class=view[i]\n",
    "        S_w_temp=np.zeros((feature_count,feature_count))\n",
    "        n_views,n_docs,n_features=ar_class.shape\n",
    "\n",
    "        for s in range(n_views) :\n",
    "            # S(th) view in class i\n",
    "            ar_view_s=ar_class[s]\n",
    "            for t in range(n_views) :\n",
    "                #t(th) view in class i\n",
    "                ar_view_t=ar_class[t]\n",
    "                doc_len_1=ar_view_s.shape[0]\n",
    "                # for each document in s(th) view\n",
    "                for p in range(doc_len_1) :\n",
    "                    doc_len_2=ar_view_t.shape[0]\n",
    "                    # for each document in s(th) view take a document in t(th) view\n",
    "                    for q in range(doc_len_2) :\n",
    "                        doc_1=ar_view_s[p]\n",
    "                        doc_2=ar_view_t[q]\n",
    "                        #The Feature Vectors are in 1D matrix of length(Number Of Features)\n",
    "                        #Converting the to 2*D matrix of Dimnesion (1 X Number Of Features)\n",
    "                        doc_1=doc_1.reshape(ar_view_s.shape[1],1)\n",
    "                        doc_2=doc_2.reshape(ar_view_t.shape[1],1)\n",
    "                        temp_prod=np.dot(doc_1,doc_2.T)\n",
    "                        S_w_temp=S_w_temp+temp_prod\n",
    "        #l(i) which represents the number of documents in class i\n",
    "        l_i=n_views*n_docs\n",
    "        S_w_temp=S_w_temp/(l_i*l_i)\n",
    "        S_w=S_w+S_w_temp\n",
    "\n",
    "    # Divide S_w by number of classes\n",
    "    S_w=S_w/(count-1)\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    #-----------------------Calculation of S_B part of equation for review--------------------\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "\n",
    "    S_B=np.zeros((number_of_features,number_of_features))\n",
    "    for i in range(n_class-1) :\n",
    "        #Extracting all the view_doc records for a class i\n",
    "        ar_class_i=class_view[i]\n",
    "        n_views_i,n_docs_i,n_features_i=ar_class_i.shape\n",
    "        for j in range(n_class-1) :\n",
    "            #If Both the Classes same Exit Loop\n",
    "            S_B_temp_class=np.zeros((number_of_features,number_of_features))\n",
    "            if i != j :\n",
    "                #Since interclass Scatter proceed with different Classes\n",
    "                ar_class_j=class_view[j]\n",
    "                n_views_j,n_docs_j,n_features_j=ar_class_j.shape\n",
    "                #For every view in class one\n",
    "                for s in range(n_views_i) :\n",
    "                    # S(th) view in class i\n",
    "                    ar_view_s=ar_class_i[s]\n",
    "                    #Inner Loop for views\n",
    "                    for t in range(n_views_j) :\n",
    "                        #t(th) view in class j\n",
    "                        ar_view_t=ar_class_j[t]\n",
    "                        #Number of Documents in View1\n",
    "                        doc_len_1=ar_view_s.shape[0]\n",
    "                        for p in range(doc_len_1) :\n",
    "                            doc_len_2=ar_view_t.shape[0]\n",
    "                            # for each document in s(th) view take a document in t(th) view\n",
    "                            for q in range(doc_len_2) :\n",
    "                                doc_1=ar_view_s[p]\n",
    "                                doc_2=ar_view_t[q]\n",
    "                                #The Feature Vectors are in 1D matrix of length(Number Of Features)\n",
    "                                #Converting the to 2*D matrix of Dimnesion (1 X Number Of Features)\n",
    "                                doc_1=doc_1.reshape(ar_view_s.shape[1],1)\n",
    "                                doc_2=doc_2.reshape(ar_view_t.shape[1],1)\n",
    "                                temp_prod=np.dot(doc_1,doc_2.T)\n",
    "                                S_B_temp_class=S_B_temp_class+temp_prod\n",
    "                #l(i) which represents the number of documents in class i\n",
    "                l_i=n_views_i*n_docs_i\n",
    "                #l(j) which represents the number of documents in class j\n",
    "                l_j=n_views_j*n_docs_j\n",
    "                S_B_temp_class=S_B_temp_class/(l_i*l_j)\n",
    "                #print (S_W_temp_class.shape)\n",
    "                S_B=S_B+S_B_temp_class\n",
    "    S_B=S_B/((n_class-1)*(n_class-2))\n",
    "    #print(\"final S_B\")\n",
    "    #print(S_B)\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    #-----------------------Calculation of S_T part of equation for review--------------------\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "\n",
    "    S_T=np.zeros((number_of_features,number_of_features))#-----Full S_T Sum-------------------\n",
    "    S_T_W=np.zeros((number_of_features,number_of_features))#-----S_W part of S_T--------------\n",
    "    for i in range(n_class) :\n",
    "\n",
    "        ar_class=class_view[i]\n",
    "        # print(i)\n",
    "        # print(ar_class)\n",
    "        S_T_W_temp_class=np.zeros((number_of_features,number_of_features))\n",
    "        n_views,n_docs,n_features=ar_class.shape\n",
    "        print(n_views,n_docs,n_features)\n",
    "\n",
    "        # Outer Loop for Each view\n",
    "        for s in range(n_views) :\n",
    "            # S(th) view in class i\n",
    "            ar_view_s=ar_class[s]\n",
    "            #Inner Loop For Views\n",
    "            for t in range(n_views) :\n",
    "                #t(th) view in class i\n",
    "                ar_view_t=ar_class[t]\n",
    "                doc_len_1=ar_view_s.shape[0]\n",
    "                # for each document in s(th) view\n",
    "                for p in range(doc_len_1) :\n",
    "                    doc_len_2=ar_view_t.shape[0]\n",
    "                    # for each document in s(th) view take a document in t(th) view\n",
    "                    for q in range(doc_len_2) :\n",
    "                        doc_1=ar_view_s[p]\n",
    "                        doc_2=ar_view_t[q]\n",
    "                        #The Feature Vectors are in 1D matrix of length(Number Of Features)\n",
    "                        #Converting the to 2*D matrix of Dimnesion (1 X Number Of Features)\n",
    "                        doc_1=doc_1.reshape(ar_view_s.shape[1],1)\n",
    "                        doc_2=doc_2.reshape(ar_view_t.shape[1],1)\n",
    "                        temp_prod=np.dot(doc_1,doc_2.T)\n",
    "                        #print (temp_prod)\n",
    "                        #print(temp_prod.shape)\n",
    "                        #Keeping Summation in S_W\n",
    "                        S_T_W_temp_class=S_T_W_temp_class+temp_prod\n",
    "        #l(i) which represents the number of documents in class i\n",
    "        l_i=n_views*n_docs\n",
    "        S_W_temp_class=S_W_temp_class/(l_i*l_i)\n",
    "        #print (S_W_temp_class.shape)\n",
    "        S_T_W=S_T_W+S_T_W_temp_class\n",
    "\n",
    "\n",
    "    #--------------Calculation of the S_B part ---------------\n",
    "    #---------------------------------------------------------\n",
    "    S_T_B=np.zeros((number_of_features,number_of_features))\n",
    "    for i in range(n_class) :\n",
    "        #Extracting all the view_doc records for a class i\n",
    "        ar_class_i=class_view[i]\n",
    "        n_views_i,n_docs_i,n_features_i=ar_class_i.shape\n",
    "        for j in range(n_class) :\n",
    "            #If Both the Classes same Exit Loop\n",
    "            S_T_B_temp_class=np.zeros((number_of_features,number_of_features))\n",
    "            if i != j :\n",
    "                #Since interclass Scatter proceed with different Classes\n",
    "                ar_class_j=class_view[j]\n",
    "                n_views_j,n_docs_j,n_features_j=ar_class_j.shape\n",
    "                #For every view in class one\n",
    "                for s in range(n_views_i) :\n",
    "                    # S(th) view in class i\n",
    "                    ar_view_s=ar_class_i[s]\n",
    "                    #Inner Loop for views\n",
    "                    for t in range(n_views_j) :\n",
    "                        #t(th) view in class j\n",
    "                        ar_view_t=ar_class_j[t]\n",
    "                        #Number of Documents in View1\n",
    "                        doc_len_1=ar_view_s.shape[0]\n",
    "                        for p in range(doc_len_1) :\n",
    "                            doc_len_2=ar_view_t.shape[0]\n",
    "                            # for each document in s(th) view take a document in t(th) view\n",
    "                            for q in range(doc_len_2) :\n",
    "                                doc_1=ar_view_s[p]\n",
    "                                doc_2=ar_view_t[q]\n",
    "                                #The Feature Vectors are in 1D matrix of length(Number Of Features)\n",
    "                                #Converting the to 2*D matrix of Dimnesion (1 X Number Of Features)\n",
    "                                doc_1=doc_1.reshape(ar_view_s.shape[1],1)\n",
    "                                doc_2=doc_2.reshape(ar_view_t.shape[1],1)\n",
    "                                temp_prod=np.dot(doc_1,doc_2.T)\n",
    "                                S_T_B_temp_class=S_T_B_temp_class+temp_prod\n",
    "                #l(i) which represents the number of documents in class i\n",
    "                l_i=n_views_i*n_docs_i\n",
    "                #l(j) which represents the number of documents in class j\n",
    "                l_j=n_views_j*n_docs_j\n",
    "                S_T_B_temp_class=S_T_B_temp_class/(l_i*l_j)\n",
    "                #print (S_W_temp_class.shape)\n",
    "                S_T_B=S_T_B+S_T_B_temp_class\n",
    "\n",
    "    #Final S_T calculation\n",
    "    S_T=S_T_B+((n_class-1)*S_T_W)\n",
    "    S_T=S_T/(2*(n_class)*(n_class-1))\n",
    "\n",
    "    r1=10\n",
    "    r2=10\n",
    "    #Calculating R\n",
    "    R=S_W-(r1*S_B)-(r2*S_T)\n",
    "\n",
    "    print(\"the R is\")\n",
    "    print (R)\n",
    "    return R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
